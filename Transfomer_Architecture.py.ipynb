{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19595173",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "818ca217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af30b6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transfomer(nn.Module):\n",
    "    def __init(self, d_model = 512, num_heads = 8, num_encoders=6, num_decoders=6):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(d_model, num_heads, num_encoders)\n",
    "        self.decoder = Decoder(d_model, num_heads, num_decoders)\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        enc_out = self.encoder(src, src_mask)\n",
    "        dec_out = self.decoder(tgt, enc_out, src_mask, tgt_mask)\n",
    "        return dec_out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d4d9c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, num_encoders):\n",
    "        super().__init()\n",
    "        self.enc_layers = nn.ModuleList(\n",
    "            [EncoderLayer(d_model, num_heads) for _ in range(num_encoders)],\n",
    "        )\n",
    "    def forward(self, src, src_mask):\n",
    "        output = src\n",
    "        for layer in self.enc_layers:\n",
    "            output = layer(output, src_mask)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4acdbe0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, num_decoders):\n",
    "        super().__init()\n",
    "        self.dec_layers = nn.ModuleList(\n",
    "            [DecoderLayer(d_model, num_heads) for _ in range(num_decoders)],\n",
    "        )\n",
    "    def forward(self, src, tgt, enc, tgt_mask, enc_mask):\n",
    "        output = tgt\n",
    "        for layer in self.dec_layers:\n",
    "            output = layer(output, enc, tgt_mask, enc_mask)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0217b8e4",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff=2048, dropout=0.3):\n",
    "        super().__init()\n",
    "        self.attn = MultiHeadedAttention(d_model, num_heads, dropout=dropout)\n",
    "        \n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        \n",
    "        self.attn_norm = nn.LayerNorm(d_model)\n",
    "        self.ffn_norm = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, src, src_mask):\n",
    "        x = src\n",
    "        x = x + self.attn(q = x, k=x, v = x, mask=src_mask)   \n",
    "        x = self.attn_norm(x)\n",
    "        x = x + self.ffn(x)\n",
    "        x = self.ffn_norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af300b1f",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout):\n",
    "        super().__init()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.attn_output_size = self.d_model// self.num_heads   \n",
    "        self.attentions = nn.ModuleList(\n",
    "           [\n",
    "             SelfAttention(d_model, self.attn_output_size)\n",
    "             for _ in range(self.num_heads)\n",
    "           ],\n",
    "        )\n",
    "        self.output = nn.Linear(self.d_model, self.d_model)  \n",
    "    \n",
    "    def forward(self, q,k,v,mask):\n",
    "        x = torch.cat([\n",
    "            layer(q,k,v,mask) for layer in self.attentions\n",
    "        ], dim=-1)\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ef42239",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, output_size, dropout=0.3):\n",
    "        super().__init()\n",
    "        self.query = nn.Linear(d_model, output_size)\n",
    "        self.key   = nn.Linear(d_model, output_size)\n",
    "        self.value = nn.Linear(d_model, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, q,k,v,mask=None):\n",
    "        bs      = q.shape[0]\n",
    "        tgt_len = q.shape[1]\n",
    "        seq_len = k.shape[1]\n",
    "        query   = self.query(q)\n",
    "        key     = self.key(k)\n",
    "        value   = self.value(v)\n",
    "        \n",
    "        dim_k   = key.size(-1)\n",
    "        scores  = torch.bmm(query, key.transpose(1,2)) / np.sqrt(dim_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            expanded_mask = mask[:,None,:].expand(bs, tgt_len, seq_len)       \n",
    "            scores = scores.masked_fill(expanded_mask == 0, -float(\"Inf\")) \n",
    "            \n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        outputs  = torch.bmm(weights, value)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efbb3a0f",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff=2048, dropout=0.3):\n",
    "        super().__init()\n",
    "        self.attn        = MultiHeadedAttention(d_model, num_heads, dropout=dropout)\n",
    "        self.masked_attn = MultiHeadedAttention(d_model, num_heads, dropout=dropout)\n",
    "        self.ffn         = nn.Sequential(\n",
    "                            nn.Linear(d_model, d_ff),\n",
    "                            nn.ReLU(inplace=True),\n",
    "                            nn.Dropout(dropout),\n",
    "                            nn.Linear(d_ff, d_model),\n",
    "                            nn.Dropout(dropout),\n",
    "                        )\n",
    "\n",
    "        self.masked_attn_norm = nn.LayerNorm(d_model)\n",
    "        self.attn_norm        = nn.LayerNorm(d_model)\n",
    "        self.ffn_norm         = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, tgt, enc, tgt_mask, enc_mask):\n",
    "        x = tgt\n",
    "        x = x + self.masked_attn(q = x, k=x, v = x, mask=tgt_mask)   \n",
    "        x = self.masked_attn_norm(x)\n",
    "        x = x + self.attn(q = x, k=enc, v = enc, mask=enc_mask)   \n",
    "        x = self.attn_norm(x)\n",
    "        x = x + self.ffn(x)\n",
    "        x = self.ffn_norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70a98af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
